{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking notebook\n",
    "This notebook contains the code to set up and run the benchmarking experiments for a variety of LLM endpoints.\n",
    "This notebook depends on the code within the module, and serves only to:\n",
    "* Queue and run experiments with consistent parameters across models\n",
    "* Process the outputs of the models into a consistent format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup\n",
    "In order to get a representative sample, we will run tests with a variety of setups:\n",
    "* Different context and completion workloads, in order to simulate a variety of customer use cases (e.g. summarisation, RAG, chat, generation)\n",
    "    * Chat: 1000 prompt tokens, 200 completion tokens\n",
    "    * Summarization: 7000 prompt tokens, 150 completion tokens\n",
    "    * Time-to-first-token simulation: 1000 prompt tokens, 1 completion token (See notes below for the reason why)\n",
    "\n",
    "Requirements/Notes:\n",
    "* An Azure OpenAI resource, with endpoints configured to use all of the resource's available TPM with no dynamic quota. In general, all other deployments should be deleted so that only the model being tested is deployed in the resource.\n",
    "* The deployment name will be used for identifying each model in the logs, so when deploying any model to an endpoint, name the deployment based on the model (e.g. \"gpt-4-8k\", \"llama-2-7b-chat\").\n",
    "* For AML Managed Online Endpoints, we cannot stream responses, so measuring the time-to-first token for a regular request will not work. To simulate this, we will instead run an additional test with the same parameters as a chat workload, however with a completion size of only 1 token. Though not perfect, this gives us some idea of the time to receive the request and pre-fill the context into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config setup\n",
    "We'll set up a series of tests for each model type, and run them one-by-one in order to collect the data.\n",
    "\n",
    "We expect to run a test for every combination of concurrency x workload profile x model deployment. Where we are testing 5x concurrency values across 3x workload profiles, each full test should take approximately 45 minutes.\n",
    "\n",
    "We'll use some helper dataclasses to hold values for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Type, Union\n",
    "from dotenv import load_dotenv\n",
    "import shlex\n",
    "import subprocess\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "# Create deployment config dataclasses that align to the different python entrypoints\n",
    "@dataclass\n",
    "class OpenAIDeploymentConfig:\n",
    "    # Dynamic config\n",
    "    api_key_env: str\n",
    "    deployment: str\n",
    "    api_base_endpoint: str\n",
    "    # Static config\n",
    "    python_entrypoint: str = \"benchmark.bench load\"\n",
    "    api_version: str = \"2023-05-15\"\n",
    "    completions: int = 1\n",
    "\n",
    "DeploymentConfigType = Type[OpenAIDeploymentConfig]\n",
    "\n",
    "# @dataclass\n",
    "# class AMLDeploymentConfig:\n",
    "#     # Dynamic config\n",
    "#     api_key_env: str\n",
    "#     deployment: str\n",
    "#     api_base_endpoint: str\n",
    "#     model: str\n",
    "#     # Static config\n",
    "#     python_entrypoint: str = \"benchmark.bench_aml load\"\n",
    "\n",
    "# DeploymentConfigType = Union[Type[AMLDeploymentConfig], Type[OpenAIDeploymentConfig]]\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    # Dynamic config\n",
    "    context_tokens: int\n",
    "    max_tokens: int\n",
    "    duration: int\n",
    "    # Static config\n",
    "    shape_profile: str = \"custom\"\n",
    "    aggregation_window: int = 60\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate config for each deployment and workload profile\n",
    "We'll create a set of Benchmark params for each workload. The workloads are based on [this document](https://microsoftapc-my.sharepoint.com/:w:/g/personal/mtremeer_microsoft_com/EfxJ8FsJcm5NmHRZuhlcRc0BC03ScNXZqKydqj3AjbIcBg?e=nmBgcF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY_VALUES = [1, 2, 4, 8, 12, 16, 20]\n",
    "\n",
    "# Run duration & rate\n",
    "WARMUP_TIME_PER_RUN_SECS = 30\n",
    "AGGREGATION_WINDOW_SECS = 60\n",
    "\n",
    "# 1. Text Generation (Output-token heavy)\n",
    "TEXT_GEN_BENCH_CONFIG = BenchmarkConfig(\n",
    "    context_tokens=200,\n",
    "    max_tokens=2000,\n",
    "    duration=WARMUP_TIME_PER_RUN_SECS+AGGREGATION_WINDOW_SECS\n",
    ")\n",
    "\n",
    "# 2. Chat, with RAG (Balanced)\n",
    "CHAT_RAG_BENCH_CONFIG = BenchmarkConfig(\n",
    "    context_tokens=3100,\n",
    "    max_tokens=300,\n",
    "    duration=WARMUP_TIME_PER_RUN_SECS+AGGREGATION_WINDOW_SECS\n",
    ")\n",
    "\n",
    "# 3. Classification (Context-heavy)\n",
    "CLASSIFICATION_BENCH_CONFIG = BenchmarkConfig(\n",
    "    context_tokens=5000,\n",
    "    max_tokens=1,\n",
    "    duration=WARMUP_TIME_PER_RUN_SECS+AGGREGATION_WINDOW_SECS\n",
    ")\n",
    "\n",
    "ALL_BENCH_CONFIGS = [TEXT_GEN_BENCH_CONFIG, CHAT_RAG_BENCH_CONFIG, CLASSIFICATION_BENCH_CONFIG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config_set(\n",
    "        deployment_config: DeploymentConfigType, \n",
    "        benchmark_configs: Iterable[BenchmarkConfig],\n",
    "        initial_warmup_secs: int,\n",
    "        concurrency_values: Iterable[int] = CONCURRENCY_VALUES,\n",
    "    ) -> Iterable[tuple[DeploymentConfigType, BenchmarkConfig]]:\n",
    "    \"\"\"\n",
    "    Combines the deployment and benchmark configs into all possible permutations, \n",
    "    plus an initial warmup config to be run prior to all benchmarks.\n",
    "\n",
    "    Returns:\n",
    "        List of execution strings to be run (in order) by the benchmarking script.\n",
    "    \"\"\"\n",
    "    # Create warmup run with 10 clients and 60 RPM @ 200 total tokens - equates to max of 1.2M TPM\n",
    "    warmup_bench_config = BenchmarkConfig(\n",
    "        context_tokens=1000,\n",
    "        max_tokens=1000,\n",
    "        duration=initial_warmup_secs\n",
    "    )\n",
    "    warmup_config = [deployment_config, warmup_bench_config, 10]\n",
    "\n",
    "    # Combine all permutations of deployment, benchmark and concurrency values\n",
    "    permutations = itertools.product(*[benchmark_configs, concurrency_values])\n",
    "    permutations = [[deployment_config, *config] for config in list(permutations)]\n",
    "\n",
    "    return [warmup_config, *permutations]\n",
    "\n",
    "def config_to_execution_str(\n",
    "    deployment_config: DeploymentConfigType, \n",
    "    benchmark_config: BenchmarkConfig,\n",
    "    clients: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Combines configs into a single execution string, ready for execution by the\n",
    "    benchmarking CLI.\n",
    "    \"\"\"\n",
    "    cmd = f\"python3 -m {deployment_config.python_entrypoint}\"\n",
    "    # Deployment config\n",
    "    for param, value in deployment_config.__dict__.items():\n",
    "        if param == \"python_entrypoint\":\n",
    "            continue\n",
    "        elif param == \"api_base_endpoint\":\n",
    "            cmd += f\" {value}\"\n",
    "        else:\n",
    "            cmd += f\" --{param.replace('_', '-')} {value}\"\n",
    "    # Benchmark config\n",
    "    for param, value in benchmark_config.__dict__.items():\n",
    "        cmd += f\" --{param.replace('_', '-')} {value}\"\n",
    "    # Logs save dir\n",
    "    subdir = \"openai\" if deployment_config.__class__.__name__ == \"OpenAIDeploymentConfig\" else \"aml\"\n",
    "    cmd += f\" --output-format jsonl --log-save-dir logs/{subdir}/{deployment_config.deployment}\"\n",
    "    # Add clients \n",
    "    assert clients > 0\n",
    "    cmd += f\" --clients {clients}\"\n",
    "    \n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT-4 PayGO - Default Content Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure API key is loaded from .env file\n",
    "load_dotenv()\n",
    "\n",
    "demo_oai_deployment_config = OpenAIDeploymentConfig(\n",
    "    api_key_env=\"OPENAI_API_KEY\",\n",
    "    deployment=\"gpt-4-1106-defaultct\",\n",
    "    api_base_endpoint=\"https://aoai-sweden-mt.openai.azure.com/\",\n",
    ")\n",
    "initial_warmup_secs = 300 # Allow 5 mins for endpoint warmup and scaling of load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 22 configs.\n"
     ]
    }
   ],
   "source": [
    "# Generate config sets\n",
    "config_set = generate_config_set(\n",
    "    deployment_config=demo_oai_deployment_config,\n",
    "    benchmark_configs=ALL_BENCH_CONFIGS,\n",
    "    initial_warmup_secs=initial_warmup_secs,\n",
    ")\n",
    "print(f\"Generated {len(config_set)} configs.\")\n",
    "\n",
    "# Convert to execution strings\n",
    "exec_strings = [config_to_execution_str(*config) for config in config_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 2 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 3 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 4 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 5 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 6 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 7 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 8 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 9 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 10 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 11 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 12 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 13 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 14 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 15 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 16 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 17 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 18 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 19 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 20 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 21 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 22 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n"
     ]
    }
   ],
   "source": [
    "for trial, exec_str in enumerate(exec_strings):\n",
    "    print(f\"Starting trial {trial+1} of {len(exec_strings)}\")\n",
    "    process = subprocess.Popen(shlex.split(exec_str), shell=False)\n",
    "    return_code = process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT-3.5 PayGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure API key is loaded from .env file\n",
    "load_dotenv()\n",
    "\n",
    "demo_oai_deployment_config = OpenAIDeploymentConfig(\n",
    "    api_key_env=\"OPENAI_API_KEY\",\n",
    "    deployment=\"gpt-4-1106-defaultct\",\n",
    "    api_base_endpoint=\"https://aoai-sweden-mt.openai.azure.com/\",\n",
    ")\n",
    "initial_warmup_secs = 300 # Allow 5 mins for endpoint warmup and scaling of load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 22 configs.\n"
     ]
    }
   ],
   "source": [
    "# Generate config sets\n",
    "config_set = generate_config_set(\n",
    "    deployment_config=demo_oai_deployment_config,\n",
    "    benchmark_configs=ALL_BENCH_CONFIGS,\n",
    "    initial_warmup_secs=initial_warmup_secs,\n",
    ")\n",
    "print(f\"Generated {len(config_set)} configs.\")\n",
    "\n",
    "# Convert to execution strings\n",
    "exec_strings = [config_to_execution_str(*config) for config in config_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 2 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 3 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 4 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 5 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 6 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 7 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 8 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 9 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 10 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 11 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 12 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 13 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 14 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 15 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 16 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 17 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 18 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 19 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 20 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 21 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n",
      "Starting trial 22 of 22\n",
      "invalid argument(s): api-key-env OPENAI_API_KEY not set\n"
     ]
    }
   ],
   "source": [
    "for trial, exec_str in enumerate(exec_strings):\n",
    "    print(f\"Starting trial {trial+1} of {len(exec_strings)}\")\n",
    "    process = subprocess.Popen(shlex.split(exec_str), shell=False)\n",
    "    return_code = process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoai_benchmark_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
